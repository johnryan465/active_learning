@misc{kirsch2019batchbald,
      title={BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning}, 
      author={Andreas Kirsch and Joost van Amersfoort and Yarin Gal},
      year={2019},
      eprint={1906.08158},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{vanamersfoort2020uncertainty,
      title={Uncertainty Estimation Using a Single Deep Deterministic Neural Network}, 
      author={Joost van Amersfoort and Lewis Smith and Yee Whye Teh and Yarin Gal},
      year={2020},
      eprint={2003.02037},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{houlsby2011bayesian,
      title={Bayesian Active Learning for Classification and Preference Learning}, 
      author={Neil Houlsby and Ferenc Huszár and Zoubin Ghahramani and Máté Lengyel},
      year={2011},
      eprint={1112.5745},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{farquhar2021statistical,
      title={On Statistical Bias In Active Learning: How and When To Fix It}, 
      author={Sebastian Farquhar and Yarin Gal and Tom Rainforth},
      year={2021},
      eprint={2101.11665},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{orthant,
author = {Nomura, Noboru},
year = {2014},
month = {07},
pages = {},
title = {Evaluation of Gaussian orthant probabilities based on orthogonal projections to subspaces},
volume = {26},
journal = {Statistics and Computing},
doi = {10.1007/s11222-014-9487-8}
}
@inproceedings{gardner2018gpytorch,
  title={GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
  author={Gardner, Jacob R and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}
@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{rasmussen2003gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Summer school on machine learning},
  year={2003},
  organization={Springer}
}


@misc{NGD,
  title = {Natural Gradient Descent},
  howpublished = {\url{https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/}},
}
@article{RKHS,
 ISSN = {00029947},
 URL = {http://www.jstor.org/stable/2693779},
 abstract = {A theorem of M. F. Driscoll says that, under certain restrictions, the probability that a given Gaussian process has its sample paths almost surely in a given reproducing kernel Hilbert space (RKHS) is either 0 or 1. Driscoll also found a necessary and sufficient condition for that probability to be 1. Doing away with Driscoll's restrictions, R. Fortet generalized his condition and named it nuclear dominance. He stated a theorem claiming nuclear dominance to be necessary and sufficient for the existence of a process (not necessarily Gaussian) having its sample paths in a given RKHS. This theorem - specifically the necessity of the condition - turns out to be incorrect, as we will show via counterexamples. On the other hand, a weaker sufficient condition is available. Using Fortet's tools along with some new ones, we correct Fortet's theorem and then find the generalization of Driscoll's result. The key idea is that of a random element in a RKHS whose values are sample paths of a stochastic process. As in Fortet's work, we make almost no assumptions about the reproducing kernels we use, and we demonstrate the extent to which one may dispense with the Gaussian assumption.},
 author = {Milan N. Lukić and Jay H. Beder},
 journal = {Transactions of the American Mathematical Society},
 number = {10},
 pages = {3945--3969},
 publisher = {American Mathematical Society},
 title = {Stochastic Processes with Sample Paths in Reproducing Kernel Hilbert Spaces},
 volume = {353},
 year = {2001}
}

