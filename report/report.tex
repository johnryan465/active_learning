\documentclass[12pt, a4paper]{report}


\input{Packages.tex}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{definition}
\newtheorem{nproof}{Proof}[section]


\hypersetup{pdftitle = Project Report, pdfauthor = {First Last}, pdfstartview=FitH, pdfkeywords = essay, pdfpagemode=FullScreen, colorlinks, anchorcolor = red, citecolor = purple, urlcolor=blue, filecolor=green, linkcolor=blue, plainpages=false}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\rhead{Computer Science}
\chead{}
\lhead{University of Oxford}
\lfoot{\date{}}
\cfoot{}
\rfoot{\thepage}
% Top and Bottom Line Rules
\renewcommand{\headrulewidth}{0.4pt} %0.4pt
\renewcommand{\footrulewidth}{0.4pt}
\fancyheadoffset{9pt}
\fancyfootoffset{9pt}
% Line spacing
\renewcommand{\baselinestretch}{1.5} %1.5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\date{}

\title{Active Learning}
\author{\\ \Large{John Ryan}
\\ Computer Science
\\
\\
\\
\\ University of Oxford
\\
\\ \\
Trinity 2021
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\DeclareMathOperator{\E}{\mathbb{E}}

\begin{document}
% Adjust logo positions here
\AddToShipoutPicture*{\BackgroundPicturea{Logos/logo2.png}{0.7in}{5.8in}}

\thispagestyle{headings}
	\maketitle
\FloatBarrier
\pagenumbering{roman}


\thispagestyle{empty}
\begin{abstract}

Active Learning is an use

\keywords{Keyword1 - Keyword2 - Keyword3}
% \vspace{-10mm} %To remove added white space after
\end{abstract}
\tableofcontents
\thispagestyle{plain}
\listoffigures
\listoftables
\listofalgocfs
\listoftheorems
\chapter*{List of Abbreviations}
\begin{abbreviations}
    \item[GP] Gaussian Process
    \item[GPC] Gaussian Process Classification
    \item[BALD] Bayesian Active Learning by Disagreement
    \item[ELBO] Evidence Lower Bound
    \item[ML] Machine Learning
\end{abbreviations}

\chapter{Introduction}
\pagenumbering{arabic}

Active learning is an area of machine learning where we are attempting to select which unlabelled datapoints we believe would be best to learn the label of. It is important because in many important use cases (for example, medical data), it is often either quite costly or just simply impossible to collect an incredibly large dataset. However current approaches to do it suffer from issues related to quality of aquired datapoints, model requirements and computational issues.

Bayesian ML models are the standard ways of building machine learning models which can calculate uncertainty well, however many of the standard approaches to doing so (MC Dropout) can give suboptimal performance, for example requiring a substantial amount of sampling. Gaussian Processes are a ML approach which give very good uncertainty performance, and new results in the field [DUE] have shown successful training of Variational Gaussian Processes with feature extractors and demonstrate that they maintain good uncertainty performance.

\section{Motivation}

Active Learning is an active area of Machine Learning where we are trying to select (unlabelled) data points in an attempt to maximize some objective.
\section{Aim and Objectives}

The aims of this report is to

\begin{itemize}
    \item Give an outline of the various methods for Active Learning which have been published
    \item Compare these methods in a reproducible and standardized fashion
    \item Extend and combine some of these approaches with other ML research to improve Active Learning performance in certain situations.
\end{itemize}
\section{Project Report Outline}
The remainder of this report is organised as follows:
\begin{itemize}
    \item[] \textbf{Chapter} \hyperref[Chap2]{\textbf{2}} --- Defines active learning, and introduces the different active learning methods from the literature.
    \item[] \textbf{Chapter} \hyperref[Gaussian Processes]{\textbf{4}} --- introduces Gaussian Process based models
    \item[] \textbf{Chapter} \hyperref[Chap5]{\textbf{5}} --- Combines BatchBALD and GPC
\end{itemize}
>

\chapter{Active Learning Methods}
\label{Chap2}

In a common active learning formulation we have the following setup.

\begin{itemize}
    \item $X_{pool}$ this is the distribution of the dataset which we have to work with in our data pool.
    \item $X_{true}$ this is the true real world distribution.
\end{itemize}

A common (and sensible) assumption is to assume that these distributions are the same.

Our machine learning models which we use to model a problem constrain the set of possible functions which we can represent and learn, this is how we imbue the problem with our prior beliefs about the nature of the problem.

- There are hard constraints (eg clipping the output of the model), which even given an unlimited amount of data our model can not possibly "learn around"
- There are softer constraints (priors over weights in a layer), which our model should be able to learn correctly given sufficient new data even if our prior is poor*.

This parameterization of our models is very important, this is an assumption we are making about our problem.

An important observation is that the dataset we are using for training is (unless performing very simple active learning approaches eg random acquisition) not going to be the same as the true dataset which we are working with. 

This violates a very common assumption that we assume for convergence in many ML methods, and is something worth keeping in mind. This statistical bias is looked at in the following paper \cite{farquhar2021statistical}.

When working with Active Learning we are also normally working with much smaller datasets than is standard in ML, our models which are normally over-parametrized to begin with become even more so, this can lead to training issues.

\section{Random}

The most straightforward acquisition function is to randomly sample from the poolset. This is a surprisingly strong baseline for for many datasets. For example with an unbalanced dataset random acquisition can fall behind as it will on average select points in the same proportion as which they occur in the pool. Here we have a comparison between unweighted MNIST and weighted MNIST with random aquisition.

\includegraphics{todo}


\section{BALD}
Bayesian Active Learning by Disagreement (BALD) \cite{houlsby2011bayesian} is an active learning method which is designed to select the datapoint which we expect would reduce the uncertainty of the posterior by the most.


$${\argmax}_x H \left[ \theta | D \right] - \mathbf{E}_{y \sim p(y | x, D)} \left[ H\left[ \theta | y, x, D\right]\right]$$

To compute this directly we would have to compute this objective in a very high dimensional space, however the authors of this paper observe that this objective can be rewritten as the conditional mutual information of the output variable and the parameters.

$$I \left(y, \theta| D, x \right) = H \left[ \theta | D \right] - \mathbf{E}_{y \sim p(y | x, D)} \left[ H\left[ \theta | y, x, D\right]\right]$$



By taking advantage of this formulation they then rewrite the objective as follows, which requires the computation of the entropy over the output variable instead which is normally over a much smaller dimensional space.


$${\argmax}_x H \left[ y | x, D \right] - \mathbf{E}_{\theta \sim p(\theta | D)} \left[ H\left[ y | x, \theta \right]\right]$$


An issue which we run into when using BALD in the case where we want to select multiple datapoints to acquire at the same time is the fact that selecting the $n$ points with the greatest individual information content will not necessarily give you the $n$ points which jointly have the greatest information content. This can be made very obvious by considering the case when there is duplicates in the dataset. If the duplicated point is in the top n points as decieded by the BALD score, having both of the points in your aquired batch will give you no more information than just having one of them, you would be better served by selecting any other point instead of these points.

\section{BatchBALD}
BatchBALD \cite{kirsch2019batchbald}, is an extension of BALD designed especially to deal with the issues which we run into when using BALD to acquire batches of points.

To do this the objective function is modified, the original BALD objective is retained by setting $n = 1$ .

The BatchBALD score function is as follows.


\begin{definition}[BatchBALD]
    $$s_{BatchBald} (x_1, \ldots, x_n) = H(y_1, \ldots, y_n) - E_{p(f)}\left[H(y_1, \ldots, y_n | f)\right]$$
\end{definition}


\begin{definition}[Submodularity]
    A function $f : P(X) \rightarrow \mathbf{R}$ is sub modular if for all $A,B \subseteq X$ we have the following.

    $$f(A) + f(B) \geq f\left( A \cup B \right) +  f\left(A \cap B \right) $$
\end{definition}

The authors of the paper prove that the objective function that they specify is \textit{sub-modular}, which enables a greedy $1 - \epsilon$ approximation algorithm for selecting the optimum batch.



As the variables are independent conditioned on the value of the function, the right hand side of the equation factors into a sum over conditional expectations for each individual datapoint. The computationally difficult part of this objective is the computation of the joint entropy as this does not factor similarly. In fact, the summation required to compute this value is exponential in the number of datapoints we are currently considering. Estimating the entropy of a discrete distribution is something which we will investigate later \hyperref[sec:Entropy]{here}.


The authors of the paper address this issue by using sampling to estimate the joint entropy for larger batch sizes. The BatchBALD paper uses Bayesian Neural networks (BNNs) as their model. They are then able to sample from the weight distribution once to obtain a function draw for all possible inputs. This is not an ability we have when using GPCs, which we will come back to later.


\section{Joint Entropy}

We can also simply use the joint entropy alone as the active learning objective

$$s_{Entropy} (x_1, \ldots, x_n) = H(y_1, \ldots, y_n)$$

\begin{nproof}[$s_{Entropy}$ is submodular]
    Proof.
    Let $A_x,B_x \subseteq D_{pool}, C_x = A_x \cap B_x$
    
    $\hat{A} = A_y \setminus C_y, \hat{B} = B_y \setminus C_y$
    \begin{align*}
        s_{Entropy} (A_x) + s_{Entropy} (B_x) &= H(A_y) + H(B_y)\\
        &= H(C_y \cup \hat{A}) + H(C_y \cup \hat{B})\\
        &= H(\hat{A} | C_y) + H(\hat{B} | C_y) + 2 H(C_y)\\
        &\geq H(\hat{A}, \hat{B} | C_y) + 2 H(C_y)\\
        &= H(\hat{A}, \hat{B}, C_y) + H(C_y)\\
        &= H(A_y \cup B_y) + H(A_x \cap B_x)\\
        &= s_{Entropy} (A_x \cup B_x) + s_{Entropy} (A_x \cap B_x)
    \end{align*}

\end{nproof}

The fact that this objective is sub-modular enables us to use the same greedy $1 - \frac{1}{\epsilon}$ approximation algorithm as used above.

A justification for the use of this objective can be seem as attempting to minimize the entropy over the remaining items in the pool given the batch which we select.


$$\min_{B} H( \left(P \setminus B\right) | B) = \min_{B} H(P) - H(B) = \min_{B} - H(B) = \max_B H(B)$$

If we assume that the samples in the poolset are samples from the true distribution, we can view this as attempting to minimize the entropy of the output variables maximally fast, this is in contrast with the BALD objective which attempts to minimise the entropy of the paramaters maximially fast.

In the next chapter we will discuss some reasons while this may be preferable when using a Gaussian Process Classifier.

\chapter{Bayesian Machine Learning Models}

Here we will detail some Bayesian Machine Learning models, in particular focusing on 2 types.

\begin{itemize}
    \item \hyperref[sec:GaussianProcesses]{Gaussian Processes}
    \item \hyperref[sec:BNNs]{Bayesian Neural Networks}
\end{itemize}


\section{Introduction}

In machine learning we are often attempting to optimise the values of some function relative to some objective. This is often done by attempting to find a single set of parameters which give the best performance. We can interpret this as attempting to find the parameters which maximize a \textit{likelihood} function, a function which represents how good of a fit to the data this set of parameters are. This likelihood function is of the form $p(x | \theta)$.


This is called \textbf{Maximum Likelihood Estimation}.

\begin{definition}[Maximum Likelihood Estimation]
    Given a likelihood function $l(x | \theta)$
$$MLE_{\theta}(x) = \argmax_{\theta} l\left(x | \theta\right)$$
\end{definition}

For example Ordinary Least Squares regression will give you the Maximum Likelihood Estimate of a linear regression model.

However if we have some belief/information about what the parameters of the model are, we can extend this approach.

\begin{definition}[Bayes Theorem]
    Let $A,B$ be events.
$$P \left( A | B\right) = \frac{P \left( B | A\right) P \left(A\right)}{P \left(B \right)}$$
\end{definition}


Define the random variables $\theta$, which represent the distribution of our parameters, and $X$ which represents our data.

$$P \left( \theta | X\right) = \frac{P \left( X | \theta \right) P \left( \theta\right)}{P \left( X  \right)}$$

Given all of these quantities we could update our beliefs about our parameters in a principled fashion.


\begin{definition}[Maximum a posteriori]
    Given a likelihood function $l(x | \theta)$
$$MAP_{\theta}(x) = \argmax_{\theta}  \frac{p \left( x | \theta \right) p \left( \theta\right)}{p \left( x  \right)}$$
\end{definition}

As $p(x)$ is going to be the same for all of the values of $\theta$ for a fixed $x$ we can ignore it in our maximization. We can view the MAP as selecting the value of $\theta$ which maximizes the likelihood of $x$, weighted by our prior belief about how likely that parameter was.


Or alternatively we can view the MLE as when we have the MAP but have a uniform belief about what the parameters are as your prior.

By selecting a single value from the parameter distribution we are discarding information, when using bayesian models we instead of simply selecting a value from this distribution we endeavour to maintain it.


\section{Bayesian Neural Networks}
\label{sec:BNNs}


Bayesian Neural Networks are a Bayesian Machine Learning model which are structured much like typical neural networks. Instead except instead of having a single output function specified by values of the parameters of the network, we maintain a distribution over the parameters of the model. This distribution and the structure of the BNN together induce a distribution over functions.

Much of the time we do not have strong priors about what the initial distributions for the weights should be.

Many of the integrals we need to perform for Bayesian inference are intractable, which unfortunately require us to perform approximation of these quantities to use these methods.

The use of dropout, a regularization method where during train time we randomly with some probability $p$ set some of the outputs from a layer to 0 can be seen as an approximate method when using BNNs.

BNNs however are limited in their expressive power via their structure, next we will look into Gaussian Processes which can approximate arbitrarily* complex function.

\section{Gaussian Processes}
\label{sec:GaussianProcesses}


\begin{definition}[Gaussian Process]
    A Gaussian processes is a collection of random variables, any finite number of which have a joint Gaussian distribution. \cite[]{rasmussen2003gaussian}
\end{definition}


We can completely specify a Gaussian Process over a space $\chi$ by giving a mean function $m(\cdot), m : \chi \rightarrow R$, and covariance (kernel) function $k(\cdot, \cdot), k: \chi \times \chi \rightarrow R$.


The output distribution for any subset of points in $\chi$ can be easily found by evaluating the mean and covariance functions, and constructing a Gaussian Distribution with these values.


\subsection{Why Gaussian Processes?}

The motivation for gaussian processes is not immediately obvious when they are first introduced, however they are an extremely powerful and useful method.

We are immediately constraining ourselves to having Gaussian valued outputs, this is often an approximation which we choose when working with more general Bayesian Methods as we often do not have any particular reason to use a different noise output.

The key reason is that Gaussian Processes can model arbitrarily* complex functions given enough data paired with a good covariance function.

It also turns out that a Gaussian Process is the limit of an infinite width feedforward neural network when the weights of the network are initialized from a Gaussian distribution. [cite]

We will return to these advantages after explaining more about how they function.

\subsection{Kernel Functions}

For $k$ to be a valid covariance (kernel) function we need for any evaluation of the covariance matrix to be a symmetric positive definite matrix (as all covariance matrices of random variables must be S.P.D).

\begin{definition}[Positive Definite Matrix]
    A positive definite matrix is a matrix which has positive eigenvalues.
\end{definition}

The kernel function of a GP defines the space of functions over which it is going to be defined.

\subsection{Gaussian Distributions}

Gaussian Distributions have some very useful properties which make them great for this particular use case.



\subsubsection{Marginalisation}

As we have defined our Gaussian Process over a space, and we require that any finite subset of the values to have a Gaussian Distribution, it is important that if we generated the GP output for $n$ points and then marginalize over one of them, we should  get the same result as if we generated the distribution over the remaining $n-1$ directly.

\begin{theorem}[Marginalisation of Gaussian Distributions]
    
\end{theorem}

This means that we don't even need to compute the marginals whenever we wish to use them, we can simply drop the relevant index from the mean and covariance.
 

\subsubsection{Conditioning}

When we want to condition one Gaussian given another Gaussian distribution we have the following result.


\begin{theorem}[Conditioning a Gaussian]

\end{theorem}


\subsection{Update Equations for Gaussian Processes}

We can use the equations above for Gaussian Distributions and extend them to our Gaussian Processes.

We can use Bayes rule to update the covariance and mean function from our prior, to the posterior given noisy of exact data. The complexity of doing this (given arbitary kernels) is bounded by matrix inversion and  multiplication of a matrix of size of the number of datapoints in which we are using. The complexity of which is sub cubic, however in general the matrix multiplication algorithms we use practically are of the cubic variety).

This can be done as follows: 

\subsection{Reproducing Kernels Hilbert Spaces}



\subsection{As distributions over functions}


We can understand this by investigating the interpretation of kernels as Reproducing Kernel Hilbert Spaces.



A positive definite kernel function can be represented by a feature space where the value of the kernel between 2 values can be computed by taking the inner product of the feature space of the 2 values.

Certain kernel functions such as the radial basis kernel, is represented by an infinite dimensional feature space. This infinite dimensional feature space enables it to model arbitarily complex functions. Polynomial or linear kernels do not enable this level of expressibility.



\subsection{Scaling to large data}

Unlike other methods we use often in machine learning, such as neural networks which have linear complexity in the size of the dataset we have greater than linear complexity.

\subsubsection{Inducing Points}

In inducing point GPs, we instead of using all of the datapoints in our model. We instead use a smaller number of datapoints as an approximation to the true model. Various approaches exist for selecting the points we keep in our approximation.

One of the most successful approaches is to treat these inducing points as model parameters which we attempt to learn by minimizing some objective.

Minimizing the evidence lower bound (ELBO) is a common and principled object for this task.

Minimizing this objective minimizes the KL Divergence between the true output distribution and the approximate distribution we are learning.

\subsubsection{Random Fixed Features}

In this approximation approach, we do not use the complete kernel matrix, but instead use a low rank approximation to the kernel bases on random fourier features. [cite]


\subsubsection{Low Rank Approximations}

Various other low rank approximation methods for the covariance have been proposed such as [], [], []



\section{Gaussian Process Classification}


To use Gaussian Processes for classification we have several issues to overcome.

The output of a Gaussian Process is a Multivariate Normal, this is a continuous distribution over the real space, however in the classification setting we wish to have a categorical output. Which means we need to have a distribution over the the simplex.

We can in theory use any link function to convert a sample from our MVN in $R^n$ to a sample over the simplex, different link functions have different properties computationally and statistically.

\subsection{Link Functions}

The link function between the latent function (our GP) and the output of our classifier is again another parameter of the model.

\subsubsection{Logit}

The logit is the standard link function for the vast majority of GPC use cases. The use of the logit implicitly makes our GPC model obey the independence of irrelevant alternatives axiom from decision theory. 


$$p\left( y = k | f\right) = \frac{e^{f^T w_k}}{\sum_{i=1}^{K} e^{f^T w_i}}$$


The marginal distribution using this link function does not have.


\subsubsection{Probit}

The probit is a different link function which is normally defined for the binary classification case as the inverse of the CDF of a Gaussian. We can generalize this link function as follows.

$$p\left(y = k | f\right) = \mathbf{1}_{argmax_i(f_i) = k}$$


The probit function is easier to work with analytically, and great use of this is made in the binary classification case \cite{houlsby2011bayesian}. However it is clear from the defintions that a single sample draw from a GPC using a probit will not give an accurate assement of the confidence of the model.


\label{sec:Integration}

If we use probit as our link function, we can take advantage of the fact that it is possible to exactly compute the marginal to get exact results.

If we can compute $p(y=k)$, we can also compute the joint entropy exactly (unlike in the case with an arbitrary link function), however the complexity of this exact computation is very high.



To perform exact inference of a Probit GPC, we must perform integration of a Gaussian R.V. This is a well studied problem in the literature. [cite].


We wish to find the probability that a certain element of a MVN is the largest element. This corresponds to integrating the Gaussian over the subset of the space where this coordinate is the largest.

$$P(X_C > X_1 \land \ldots \land X_C > X_n) = \int_{x_c \in \{ -\infty, \infty \} } \ldots \int_{x_n \in \{ -\infty , x_c \}} p(x) dx$$

This can be interpreted as integrating on one side of several hyperplanes.

These planes are of the form $x_c = x_i$

We can perform a linear transformation on this space to project it to a subspace of dimension $n-1$, and align each of the hyperplanes with an axis.

$Z = TX$.

$Z \sim N(T \mu, T \Sigma T^T)$


If we are doing this over multiple datapoints, we can transform each of the subsets of variables via the method above.

After performing this transformation we have a standard orthant integral which we can compute. Algorithms exist for computing this exactly in $O(p^2 2^p)$ using recursive integration and subspace projection. \cite{orthant}


However the number of dimensions which we are performing this integration over is $O(dc)$. With the exponential complexity in the number of dimensions this is an infeasible method.



\section{Training}

Using a straightforward Gaussian Process with no hyper parameters, for regression there is nothing to train. We simply have the exact equations to compute the posterior distribution at any possible input. In contrast to many optimization problems our output for any particular value is not a point estimate, it is a distribuiton so our objective should take this into account.


\subsection{KL Divergence}

The KL Divergence is a measure of how different 2 probability distributions are, (it is not symmetric so it is not strictly a distance functions). It has many useful properties and unlike shannon entropy it has a sensible and natural extension to continous random variables.

\begin{definition}[KL Divergence]
    For 2 probability distributions $P, Q$ defined on the same probability space $\chi$
    $$D_{KL} \left( P || Q \right) = E_{x \sim P(x)}  \log \left( \frac{P(x)}{Q(x)}\right)$$
\end{definition}



It is often quite difficult to compute the \textbf{KL Divergence}, as often we don't have direct access to one of the distributions which we are attempting to calculate it with.

The Evidence Lower Bound is a tractable objective which it is much easier for us to compute than the KL divergence and maximizing this objective is equivalent to maximizing the KL Divergence.

\begin{definition}[ELBO]
    
\end{definition}


\begin{proof}[Maximisng the ELBO will minimise the KL Divergence]
    \begin{align*}
        D_{KL} \left( Q || P \left(L | X\right) \right) &= 
    \end{align*}
\end{proof}


\chapter{vDUQ / DUE}

vDUQ / DUE \cite{vanamersfoort2020uncertainty} is a recent model which deals with some of the issues with using deep kernel learning with GPCs. DUE uses an inducing point GP to enable it to scale to large amounts of data.

This model is designed to have good uncertainty properties by enforcing sensitivity and bounding the amount by which any 2 inputs can diverge in the feature space. These properties are maintained by the additon of residual connections to the model and by performing spectral normalisation to bound the lipshitz constant of the layers in the feature extractor.

\section{Spectral Normalization}

\begin{definition}[Lipshitz Constant]
    Given a function $f: X \rightarrow Y$, where $(X, d_x)$ and $(Y, d_y)$ are metric spaces. A lipchitz constant of the function $f$ is a value of $k$ such that.

    $$\frac{d_y(f(x_1), f(x_2))}{d_x(x_1, x_2)} \leq K, \forall x_1, x_2 \in X $$
\end{definition}

Some times we refer to \textit{the lipchitz constant} of a function, this is normally taken to mean the smallest value $k$ which is a lipchitz constant.

In our particular problem we are dealing with spaces in $\mathbf{R}^n$. The metric on this space which we will be using is the $L_2$ norm, (also known as the euclidean distance).

The lipchitz constant is a measure of how far apart 2 outputs to the function can be in terms of their inputs.

In the case where $f: R^n \rightarrow R^m$ and $f$ is a linear transformation we get a familiar result.

\begin{align*}
    \min_k  \forall x_1,x_2 \in X \frac{d_y(f(x_1), f(x_2))}{d_x(x_1, x_2)} \leq K &= \min_k \forall x_1,x_2 \in X \frac{ L_2(Ax_1 - Ax_2)  }{ L_2(x_1, x_2)} \leq K \\
    &=  \min_k \forall x_1,x_2 \in X  \frac{ ||A(x_1 - x_2)||_2  }{ ||x_1- x_2||_2 } \leq K \\
    &=  \min_k \forall x \in X  \frac{ ||Ax||_2  }{ ||x||_2 } \leq K \\
\end{align*}

After the substitution we obtain an expression which from linear algebra we know is the matrix norm of $A$ induced by the vector norm which we have chosen, in this case the $L_2$ norm. The induced matrix norm by the $L_2$ vector norm has the nice property that it is the \textit{largest singular value} of the matrix which is also the square root of the largest eigenvalue.

For all of the linear layers in our model we now know how we can compute the respective lipchitz constants.

\begin{theorem}[Lipschitz constant of function compositions]
    If $lip(f) = a$ and $lip(g) = b$, then $lip(f \circ g) \leq a * b$
\end{theorem}

It turns out that the majority of activation functions which we use in our neural networks have a lipchitz constant of $\leq 1$, eg relu. This enables us to compute an upper bound of the lipschitz constant of our model by computing the lipschitz constant of all of the indiviudal layers and taking the product of those constants.



\subsection{Power Method}

Directly computing the largest singular value via computing the Singular Value Decomposition of each linear transform is quite computationally prohibitive, however there is a way in which we can approximate this value quite cheaply and accurately.

The \textbf{power method} is defined by a recurrence relation.

\begin{definition}[Power Method(for square matrices)]
    $$v_{k+1} = \frac{Av_k}{||Av_k||} $$
\end{definition}

This recurrence relation will converge if 2 conditions are met.

\begin{itemize}
    \item $A$ has a strictly largest eigenvalue
    \item $v_0$ has a non zero component in the direction of the largest eigenvalue
\end{itemize}

We compute the eigenvalue from the rayleigh quotient from our approximate maximum eigenvector. $\frac{v_k^T A v_k}{v_k ^ T v_k}$

If we chose a starting vector $v_0$ with uniform probability over all possible directions the second condition will be fulfilled with a probability of $1$.

The convergence ratio of this algorithm is $\frac{|\lambda_1|}{|\lambda_2|}$, so the convergence rate depends on the size of the second largest eigenvalue also.


To extend this to work with non square matrices we can either compute $A^TA$ or $AA^T$, use the power method to calculate its maximum eigenvalue and this is the spectral norm of $A$.
There is another approach which enables us to bypass this computation by instead of maintaining just 1 eigenvector, we maintain a left and a right dominant eigenvector

\begin{definition}[Power Method(for non square matrices)]
    $$v_{k+1} = \frac{A^Tu_k}{||A^Tu_k||}, u_{k+1} = \frac{Av_k}{||Av_k||} $$
\end{definition}

Then with these approximate eigenvectors we can approximate the singular value with $u^T A v$

To bound the lipchitz constant of a linear layer to a values, call this value $\sigma$. We can calculate the spectral norm $\alpha$, check if $\alpha > \sigma$. If this is true, we muliply the linear layer by $\frac{\sigma}{\alpha}$.

To maintain this during training we are required to compute this value during every forward pass. Due to the fact that given a sufficiently small learning rate we expect that the change to linear layer will be small, if we use the previous forwards passes final vector(s) as our starting vector instead of randomly restarting we can greatly speed up the convergence of the recurrence relation. We find that empircally we can actually get very good performance with a single iteration of this method at each stage.


\section{Residual Connections}

Residual connections were introduced in by [cite]. The indention of these connections in deep learning is to minimize the vanishing gradient problem. This is an issue that when we have particualy

$$H(x) = F(x) + x$$


\section{Modifications}

While DUE is an extremely powerful model, some modifications were required to help the performance and stability of the model when working with small amounts of data. 

\subsection{Natural Gradient Descent}

Natural Gradient Descent is an optimization method which is designed for when we are attempting to optimise some objective of a distribution. The use of this method greatly improved the rate of convergence and overall performance. \cite{NGD}


\begin{definition}[Gradient Descent]
    Gradient Descent is an iterative optimization algorithm where we modify the parameter in the direction which has the steepest gradient with respect to our objective function.
    $$p_{k+1} = p_{k} -  \alpha \nabla L(p_k)$$
\end{definition}

With a well chosen hyperparameter $\alpha$ and a convex loss function $L$ we can obtain convergence guarantees. [cite]. Many machine learning problems which we use gradient descent for do not obey the constraints which we require for guarentees of convergence [], empircally however the performance achieved by gradient descent and its derivates is impressive even these problems in many cases.

Instead of applying the gradient descent algorithm to the parameters in the euclidean geometry of the parameter space, instead we can use the geometry of the likelihood space paramaterised by our chosen parameters to enable superior optimization. This makes the optimisation independent of the paramaterisaton of the distribuiton and only related to the distribuiton induced by the the parameters.
 
To derive the update equations we will need to define some objectives and take some taylor expansions.

The first thing we will define is out loss function. $q$ is the true distribuiton which we are attempting to approximate, $p_\theta$ is our parameterized distribution.

$$L \left(\theta \right) = D_{KL} \left(p_\theta || q \right) $$

We wish to choose a descent direction which will minimize our loss function, much like in standard gradient descent. Normally in gradient descent we bound the magnitude of the step of the gradient by the eucliedean length of the descent vector, we change the constraint in natural gradient descent to instead bound the $KL$ divergence between $p_\theta$ and $p_{\theta + \epsilon}$.

\begin{align}
    \epsilon^\prime &= \argmin_{\epsilon \, s.t \, D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) \leq k} L \left( \theta + \epsilon \right) \\
\end{align}

We can take the first order taylor expansion of $L\left(\theta + \epsilon\right)$ around $\theta$.


\begin{align*}
L\left(\theta + \epsilon\right) &\approx L\left( \theta \right) + J_L \left( \theta\right) \left( \left( \theta + \epsilon \right) - \theta \right)  \\
&\approx L\left( \theta \right) +  J_L \left( \theta\right) \epsilon \\
\end{align*}

Given the constraints above we have we can rewrite our objective using the method of lagrange mulitpliers as below.

\begin{align}
    \epsilon^\prime &= \argmin_{\epsilon \, s.t \, D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) \leq k} L \left( \theta + \epsilon \right) + \alpha \left( D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) - k\right) \\
    &\approx \argmin_{\epsilon \, s.t \, D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) \leq k}  L\left( \theta \right) +  J_L \left( \theta\right) \epsilon  + \frac{1}{2} \left(\epsilon \right)^T H_L \left(\theta \right) \left( \epsilon\right) + \alpha \left( D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) - k\right) \\
\end{align}

Now we can apply a taylor expansion to $D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right)$ around $\theta$. $\theta^\prime = \theta + \epsilon$


\begin{align}
    D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) &\approx  D_{KL} \left(p_\theta || p_{\theta} \right) + \left(J_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right) \right) \epsilon + \frac{1}{2} \epsilon^T \left( H_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right) \right) \epsilon
\end{align}

The KL Divergence between a distribution and itself is $0$ so the first term disappears.


Lets now deal with the second term. The KL Divergence is a positive function, and we know that as we mentioned above it is $0$ when the 2 distributions are equal. Then second term is the Jacobian of the KL Divergence between 2 paramaterised distributitons, \textit{equaluated when the parametrs are equal}. So we are taking the Jacobian of a function evaluated at a global minimum, which implies that $J_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right)  = 0$. This makes the second term disappear.


\begin{align}
    D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) &\approx  \frac{1}{2} \epsilon^T \left( H_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right) \right) \epsilon
\end{align}


We can expand out the matrix in the remaining quadratic term to obtain that it is equal to the negative fisher information matrix.

\begin{align}
    H_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right)_{ij} &= J \left( J \left( E_{x \sim p_\theta(x)}  \log \left( \frac{p_\theta(x)}{p_{\theta^\prime}(x)}\right)\right)^T \right)_{ij} \left( \theta \right) \\
    &= \frac{\partial}{\partial \theta^\prime_i} \frac{\partial}{\partial \theta^\prime_j} E_{x \sim p_\theta(x)}  \log \left( \frac{p_\theta(x)}{p_{\theta^\prime}(x)}\right) \left( \theta \right) \\
    &= E_{x \sim p_\theta(x)} \frac{\partial}{\partial \theta^\prime_i} \frac{\partial}{\partial \theta^\prime_j}  \log \left( \frac{p_\theta(x)}{p_{\theta^\prime}(x)}\right) \left( \theta \right) \\
    &= - E_{x \sim p_\theta(x)} \frac{\partial}{\partial \theta^\prime_i} \frac{\partial}{\partial \theta^\prime_j}  \log \left( p_{\theta}(x) \right)  \\
    &= F \\
\end{align}

Where $F$ is the definition of the Fisher information matrix.


\begin{definition}[Fisher Information Matrix]
    $$\mathcal{I} \left( \theta\right)_{ij} = - E \left[ \frac{\partial^2}{\partial \theta_j  \partial \theta_i }  \log p \left(x  \right)\big| \theta \right]$$
\end{definition}

Returning to our objective from earlier.

\begin{align}
    \epsilon^\prime &\approx \argmin_{\epsilon \, s.t \, D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) \leq k}  L\left( \theta \right) +  J_L \left( \theta\right) \epsilon  + \alpha \left( \frac{1}{2} \epsilon^T F \epsilon - k\right) \\
\end{align}

To find the best value of $\epsilon$, we can take the derivate of the objective with respect to $\epsilon$ at set it to $0$.


\begin{align}
   \frac{\partial}{\partial \epsilon} \left( L\left( \theta \right) + J_L \left( \theta\right) \epsilon + \alpha \left( \frac{1}{2} \epsilon^T F \epsilon - k\right) \right) &=  \frac{\partial}{\partial \epsilon} \left( J_L \left( \theta\right) \epsilon + \frac{\alpha}{2} \epsilon^T F \epsilon  \right) = 0\\
  0 &=  J_L \left( \theta\right)^T + \frac{\alpha}{2}  \left(F^T + F \right)\epsilon\\
\end{align}

Assuming some smoothness conditions we get that the order of the partial differentiation doesn't impact the Hessian, and in turn the fisher information matrix. $F = F^T$

\begin{align}
   0 &=  J_L \left( \theta\right)^T + \alpha F \epsilon\\
   - J_L \left( \theta\right)^T &=  \alpha F \epsilon\\
  \epsilon &=  - \frac{1}{\alpha} F^{-1} J_L \left( \theta\right) ^T
\end{align} 

We have now arrived at the update equations which we can use as our optimization method.


\begin{definition}[Natural Gradient Descent]
    Natural Gradient Descent is similar to gradient descent expect in place of the the gradient we use the natural gradient defined as $\nabla^{\prime} =  F^{-1} J_L \left( \theta \right)^T$
\end{definition}






\chapter{Active Learning with DUE}
\label{Chap5}



\section{Comparison of BALD and Entropy for DUE}

The motivation for the BALD objective in the original paper where it was introduced \cite{houlsby2011bayesian}, is to select the point which maximally reduce the uncertainty over the hypothesis space.


Lets consider 2 types of points in which our model is unsure about.

Category 1, strange, lets call the set of these points $S$. These are points which are far from the training dataset in the feature space.

Category 2, confusing, lets call the set of these points $C$. These are points which are near the training dataset in the feature space but are near to multiple different categories in this space, the model does not have a clear idea of what class this datapoints is.



\subsection{BatchBALD}
For the points in $S$, the model will revert to the prior (as we are far from the dataset), the covariance between these points and most others will be quite small also.

This small covariance will lead to the conditional entropy of the datapoint given the other previously selected points being quite similar to the entropy of the datapoint alone.

While this datapoint is evidently strange, being far away from other points in the feature space. The BatchBald score for this datapoint will be quite small.

For points in category $C$ this problem does not occur.

This means that when we are using the BatchBALD as our objective, we will be first acquiring the most confusing points.


This behavior is observed when training with the BatchBALD objective. We observe error oscillations, where we tend to select datapoints which are ambiguous between categories.

\subsection{Joint Entropy}
When using the joint entropy of the variables we get that the points in category $S$ will be close to uniform, (as all of our GP outputs have the same mean).

This will give these points high entropy, this means that if we use this objective we would be sampling primarily from points in the set $S$ first, before points in the set $C$.

This means that when we are using the joint entropy as our objective, we will be first acquiring the most diverse points and then we would begin to select the more confusing points as they would still have higher entropy than well represented points.


\subsection{Why is this different?}

This is not an issue which is observed in \cite{kirsch2019batchbald} where the BatchBALD objective is used or in \cite{houlsby2011bayesian} where the BALD objective is used with GPCs. 

In \cite{kirsch2019batchbald}, the key reason is the use of BNNs. This property where the covariance between 2 points is related via some vaguely distance preserving function does not exist. This means that for points far away from the training data, the conditional entropy will not necessarily be uniform leading to the subtracted value being significantly smaller than in our case.

But how do we reconcile this with the original paper \cite{houlsby2011bayesian}, where Gaussian Process Classification is used?


The artificial datasets which are used in this paper are quite low dimensional, (3 of the experiments are in 2 dimensions), in contrast the size of the feature space we are working with is 1024 dimensional. The expected euclidean norm between random points in higher dimensional space increases quite quickly, which will lead to a lower covariance between datapoints.

The kernel indicates how far to we should extrapolate. Additionally when we are applying a GPC to a fixed dataset, we can select the relevant hyperparameter quite well based on the scale of the data. However in DUE we are applying this to a learned feature space, for which these hyperparameters can not be easily determined in advance and the optimum value would constantly change throughout training.


Given we have intentionally designed our model to not extrapolate too far beyond the dataset we have seen, it seems clear that to improve performance on the datapoints which are far from all of the training datapoints we will need to acquire some of these points and that simply selecting the points which enable the model to distinguish between ambiguous classes alone will not lead to the best active learning performance. Given MNIST has many examples which are quite difficult for even a human to distinguish, there is no shortage of datapoints in the $C$ class.


\section{Sampling}

The active learning score functions which we have specified earlier in this report require us to sample from the underlying distributions of our GPC.

Given the difficultly of performing exact computation of the quantities required for the BatchBALD objective, the general approach is to use methods which rely on sampling.

In comparison with the approach performed in the BatchBALD paper where sampling is done over the weights of the Bayesian Neural Network, when we have a Gaussian Process as our output. It is not as straightforward to take a sample function from our postior. If we are to take a sample function output over a set out points, we would need to compute the joint distribution of these points and then take a sample from this distribution and pass this through our link function.
The complexity of performing this is cubic in the size of the pool, this complexity is unacceptably high as we are often working with quite large pool sizes.

This means that we can not use the same computational tricks used previously to speed up these calculations, however we can take advantage of other properties of Gaussian Processes to make some improvements in this aspect.


\section{Entropy}
\label{sec:Entropy}

\begin{definition}[Entropy]
    For a discrete r.v $X$ the entropy is defined as $$H(X) = - \sum_{x \in X} p(x) \log p(x) $$
\end{definition}


We require the ability to compute the joint entropy of a large number of random variables to compute several of the score functions which we are evaluating.

\subsection{Gaussian Process Classifier}
As we are using a Gaussian Process Classifier, the random variable of which we are trying to estimate the value of is of a known form.


$$ \mathbf{f} \sim N(\mu, \Sigma) $$
$$ \mathbf{p} =  \sigma \left(f \right) $$
$$ X_i \sim Cat(p_i) $$

The distribution $p(x)$ does not have a closed formula. We can estimate $p(x)$ via estimating the integral via monte carlo integration.

When we are considering larger and larger pool sizes the number of possible states of our random variable grows at an exponential rate.

With $C$ classes the entropy sumation will have $C^n$ terms in it. 

As we have that $ X_i \perp\!\!\!\perp X_j | f$, for a given likelihood sample we can store the exponential number of terms in a factored form. We for a sample $f$, we can compute $p(x_i = c | f)$ for each of the datapoints. To reconstruct the joint distribution for this function sample we take the product of the terms and relevant categories.


$$ p(X) = p(x_1, \ldots, x_n) = \E_{l \sim p(l)}  p(x_1, \ldots, x_n | l) = \E_{l \sim p(l)} \prod_{i=1}^n  p(x_i| l)$$

\subsection{Estimating the entroy}

The are 2 different approaches we can use in the estimation.

\subsection{Taking samples from the distribution}

We can take samples from the distribution, and then use these samples to estimate the entropy.



\subsubsection{Plugin Estimator}
The most straightforward approach is to use the \textit{plugin estimator}. This approach is to take $k$ samples from the distribution of interest, calculate the observed probability of each class and compute the entropy.

As we define the term in the entropy summation as $0$, when the probability is $0$. Using this approach bounds the number of terms in the summation by $k$.

$$p_j = \frac{1}{k} \sum_{i=1}^k \mathbf{1}_{X = j} $$

$$\hat{H}_k = - \sum_{i=1}^{c} \hat{p}_i \log{\hat{p}_i}$$


Some of the convergence properties of the plugin estimator are below.
$$\E \left(H - \hat{H}_k \right)^2 = O \left ( \frac{1}{k} \right) $$

$$ \frac{\sqrt{K}}{\sigma} \left(H - \hat{H}_k \right) \sim \mathbf{N} \left(0, 1 \right) $$



\subsubsection{LP Estimator}

Using an \textit{LP Estimator} [cite] we can estimate the the entropy with far less samples in comparision to using a plugin estimator. The required number of samples from the distribution to estimate the entropy well is \textit{sublinear}, of the order $O \left( \frac{d}{\log{d}}\right)$, where $d$ is the number of values in the domain which have positive support. However the amount of values with support in our problem is going to be exponential as our final layer is a softmax, which means that $d = c^n$.


\subsection{Sampling from the outer summation}

As we can actually compute the values $p(x)$ without sampling from the final categorical distribution, we can take advantage of this fact to improve the performance of our estimator. We can directly obtain the probabilities after the softmax. This enables us to perform the estimation in 2 steps.

\begin{itemize}
    \item Sample from the outer summation to see which values of $p(x)$ we are required to compute.
    \item Estimate $p(x)$ directly from the output of the softmax.
\end{itemize}

We can also take advantage of the fact that the output variables are conditionally independent given a function sample. This enables us to sample from $p(x)$ without requiring an exponential amount of computation by sampling from the distribution in the following the following sequence of steps.

\begin{itemize}
    \item Sample from the likelihood distribution $p(l)$
    \item Put this sample though the softmax
    \item As the outputs of the different variables are conditionally independent given $l$, we sample once from the categorical distribuiton for each variable.
    \item This is a sample from $p(x)$
\end{itemize}

\chapter{Computation}
\label{sec:Computation}


\chapter{Implementaion}
\label{sec:Implementaion}

All of the experiments referenced here can easily be replicated using the codebase for this project which is linked and details in the appendices.

The code for this project is written in Python and makes significant use of the frameworks \cite[Pytorch]{NEURIPS2019_9015} and \cite[GPytorch]{gardner2018gpytorch}.





\section{Computational Complexity}

In this project we also endevour to minimise the computationally complexity of the operations which we need to perform to compute the quantities involved to make this project of practical use.


\subsection{Sampling from an Multivariate Distribution}


During the creation and sampling of the Multivariate distributions we are required to compute several quantities.

We generate samples from MVNs by generating samples from the identity mvn, and then transforming these samples.

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwFunction{CreateSamples}{CreateSamples}
    \CreateSamples{$\mu, \Sigma$}{
    \KwResult{Sample from $ N \left(\mu, \Sigma \right)$ }
    $n = len(\mu)$\;
    $v = array[n]$\;
    $i = 0$\;
    \While{$i < n$}{
        $v[i] = mvnsasmple()$\;
        $i++$\;
    }
    $L = cholesky(\Sigma)$\;
    \KwRet $\mu + L v $\;
    }
    \caption{Sampling from a Multivariate Gaussian}
\end{algorithm}

To sample from a distribuiton of size $n$, if the cholesky distribution is already computed this requires $O(n^2)$ operations. If it is not it requires $O(n^3)$. This encourages us to attempt to minimise the number of distint times where we must compute the cholesky distribuiton.


\subsection{Sampling over our candidate batchs}

Computing samples for each possible candidate batch at each aquisition naively would lead to a complexity at each stage of $O\left(n k (d \times c)^3 \right)$ for sampling, where $k$ is the number of samples, $c$ is the number of categories, $n$ is the number of points in the pool and $d$ is the size of the current candiate batch.

If we cache the cholesky distributions we can reduce the complexity to $O(n (d \times c)^3  + n k (d \times c)^2)$.

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwFunction{CreatePoolSamples}{CreatePoolSamples}
    \CreatePoolSamples{$\mu, \Sigma$}{
    \KwResult{$K$ samples from each of the $N$ candiate batches}
    $v = array[N][K][D][C]$\;
    $i = 0$\;
    \While{$i < N$}{
        $j = 0$\;
        \While{$j < K$}{
            $v[i][j] = CreateSamples(\mu_{i}, \Sigma_{i})$\;
            $j++$\;
        }
        $i++$\;
    }
    \Return{$v$}\;
    }
     \caption{Sampling from all possible batches}
\end{algorithm}

This level of complexity is unacceptable, however we can take advantage of the structure of our model to improve the efficency.

As the Gaussian Process Model we are using is an \textit{independent multitask model}. This means that our covariance matrix is a block diagonal. We can take advantage of this to instead of working with covariance matrices of size $ cd \times cd $, we can work with a batch of matrices of size $c \times d \times d$ in our computaion.


This reduces the complexity of the sampling to $O(n c (d)^3  + n k c (d)^2)$.

\subsection{Conditional Distributions}

We can reduce the amount of computation required by sharing computation between datapoints.

We can do this as follows first by sampling from the current batch, then sampling from the conditional distribuiton of each of the datapoints. The size of the current batch will increase as our aquisitions progress, the size of the consitional distribuitons will not.


\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwFunction{CreatePoolSamplesEfficent}{CreatePoolSamplesEfficent}
    \CreatePoolSamplesEfficent{$\mu, \Sigma, \mu_{batch}, \Sigma_{batch}$}{
    \KwResult{$K$ samples from each of the $N$ candiate batches}
    $v = array[N][K][D][C]$\;
    $samples = array[K][D][C]$\;
    $a = 0$\;
    \While{$a < K$}{
        $samples[a] = CreateSamples(\mu_{batch}, \Sigma_{batch})$\;
        $a++$\;
    }
    $i = 0$\;
    \While{$i < N$}{
        $j = 0$\;
        \While{$j < K$}{
            $\mu_{cond dist}, \Sigma_{cond dist} = CreateConditional(\mu_i, \Sigma_i, samples[j])$\;
            $v[i][j] = CreateSamples(\mu_{cond dist}, \Sigma_{cond dist})$\;
            $j++$\;
        }
        $i++$\;
    }
    \Return{$v$}\;
    }
     \caption{Sampling from all possible batches}
\end{algorithm}

$CreateConditional$ will require a $O(1)$ matrix vector products each with a complexity of $O(cd^2)$, and we will require the matrix inverse of the current batch covariance matrix. This will be the same over all function calls so we don't need to recompute this value.

The complexity of this algorithm is $O\left(cd^3 + kcd^2 + nkcd^2 \right)$.

The improvement here is to disconnnect the size of the poolset from the cubic $d^3$ term.

The single cubic term can also be removed by performing a rank-1 update on the inverse of the previous batch. This can be performed by using a block matrix inversion identity.

$$\begin{bmatrix}
    A & B \\
    C & D \\
\end{bmatrix}^{-1} = \begin{bmatrix}
    A^{-1} + A^{-1} B \left(D - C A^{-1} B \right)^{-1} C A^{-1} & -A^{-1} B \left( D - C A^{-1}B\right)^{-1} \\
    - \left(D - CA^{-1}B \right)^{-1} C A^{-1} & \left(D - C A^{-1} B\right)^{-1}
\end{bmatrix} $$ 

Substitutioning in our rank 1 format.

\begin{align*}
    \begin{bmatrix}
        A & u \\
        u^T & \alpha \\
    \end{bmatrix}^{-1} &= \begin{bmatrix}
        A^{-1} + A^{-1} u \left(\alpha - u^T A^{-1} u \right)^{-1} u^T  A^{-1} & -A^{-1} u \left( \alpha - u^T A^{-1}u\right)^{-1} \\
        - \left(\alpha - u^T A^{-1}u \right)^{-1} u^T A^{-1} & \left(\alpha - u^T A^{-1} u\right)^{-1}
    \end{bmatrix}
\end{align*}

If we compute the upper quadrant of the result in such an order that we compute matrix vector products until we are left with a outer product, this will give us a $O(d^2)$ matrix inverse update.


This method gives us a complexity of  $O\left(nkcd^2 \right)$ for the entire sampling.


We can once again reduce the amount of computaion required to take $K$ samples from each candidate by taking $S = K // M$ samples from the batch and taking $M$ samples from the conditional distribuitons.


If we do this our complexity will become $O\left(scd^2 + nscd^2 + nsmc\right)$.

\subsection{Creation of GP Outputs required}

To be able to compute the samples as we have above, at each stage of the aquisition we require the covariance of each point in the candidate batch and the pool points. To compute this at each batch batch directly we end up requring $O(ncd^3)$ computation to evaluate the covariance matrices. However there is clearly wasted computation as all of the candidate batches will have an identical submatrix of size $d \times d$.

The approach taken in this project is as follows. We at each stage compute the GP output of the most recently aquired datapoint with each of the datapoints in the pool. The complexity of this operation is $O(nc)$, as we are only computing covariances between pairs of points.

We can then cache these computaions as the aquisitions progress, we can compute all of the possible rank-1 updates to the current batch in time $O(ncd)$, from these stored values. This once again eliminates the cubic term and replaces it with a linear one.


Both the conditional distribuitons and the rank 1 updates are lazily generated to reduce the memory usage of the program, in addition to the computations are dynamically chunked to enable them to fit in memory and to be able to execute with limited RAM. This is essential to obtain good performance using CUDA operations on a GPU, as the amount of VRAM available limits the amount of parallel computaion one can take advantage of.

\subsection{Entropy}



\section{Replication}

To begin with replication of some of the experiments in the BatchBALD to valididate the experimental framework and to have points of comparision to work with.




\section{DUE}

\subsection{Model Outline}


\subsubsection{Training issues}

When training DUE/vDUQ based models on extremely small datasets, an issue which begins to become apparent is the issue of inducing point initialisation. As feature extractor is randomly initialised, the initial inducing points we get obtain by using standard initial inducing point initialisation procuedures can lead to poor training dynamics.





\subsection{Sampling Functions}

As mentioned earlier in the paper sampling a function over the entire pool is computationally prohibative, however we can take a small enough subset of the pool over which we can calculate the joint distribution and take sample function draws.
We can then use these as a numerical test to valididate that the more computationally efficent methods we develop are giving correct results.


\chapter{Experiments}

We have set up some of these experiments to demonstrate the issues with some of the active learning methods detailed in this paper.

The random method should be impacted by how unbalanced the dataset is particuallly.

\section{Replication}

\subsection{Random}

\subsubsection{MNIST (DNN vs BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{Random MNIST}
\end{figure}

\subsubsection{Unbalanced MNIST (DNN vs BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{Random Unbalanced MNIST}
\end{figure}
\subsubsection{Repeated MNIST (DNN vs BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{Random Repeated MNIST}
\end{figure}

\subsection{BALD}

\subsubsection{MNIST (BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{BALD MNIST}
\end{figure}

\subsubsection{Unbalanced MNIST (BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{BALD Unbalanced MNIST}
\end{figure}

\subsubsection{Repeated MNIST (BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{BALD Repeated MNIST}
\end{figure}
    

\subsection{BatchBALD}

\subsubsection{MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\subsubsection{Unbalanced MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\subsubsection{Repeated MNIST (BNN vs DUE)}

\includegraphics{todo.png}


\subsection{Predictive Entropy}

\subsubsection{MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\subsubsection{Unbalanced MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\subsubsection{Repeated MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\section{DUE}

To attempt to keep the experiments as comparible as possible, the feature extractor architecture used for DUE for this problem was the same architecture as the BNN with minimal modifications to satify the constraints of DUE. This primarily consisted of the additon of residual connections to the models, and the removal of the dropout layers.


\subsection{Training on small datasets}

Training a machine learning model on a small dataset can exaherbate many issues that we run into in machine learning. For example the models we have are even further over parameterised when we are operating in very small datasets.

While DUE has no problems in learning MNIST with more expressive models, eg more traditional ResNets. When using these the performance on very small datasets was reduced significantly, the performance of DUE on small subsets of MNIST was quite significatnly impacted.


\subsection{BALD}

\subsection{BatchBALD}

\subsection{Predictive Entropy}






\renewcommand{\bibname}{Bibliography}
\bibliographystyle{apacite}
\bibliography{Bibliography.bib}

\begin{appendices}
\chapter{Appendix}

\end{appendices}

\end{document}