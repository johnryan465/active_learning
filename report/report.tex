\documentclass[12pt, a4paper]{report}


\input{Packages.tex}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{proof}{Proof}[section]
\newtheorem{theorem}{Theorem}[section]

\hypersetup{pdftitle = Project Report, pdfauthor = {First Last}, pdfstartview=FitH, pdfkeywords = essay, pdfpagemode=FullScreen, colorlinks, anchorcolor = red, citecolor = purple, urlcolor=blue, filecolor=green, linkcolor=blue, plainpages=false}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\rhead{Computer Science}
\chead{}
\lhead{University of Oxford}
\lfoot{\date{}}
\cfoot{}
\rfoot{\thepage}
% Top and Bottom Line Rules
\renewcommand{\headrulewidth}{0.4pt} %0.4pt
\renewcommand{\footrulewidth}{0.4pt}
\fancyheadoffset{9pt}
\fancyfootoffset{9pt}
% Line spacing
\renewcommand{\baselinestretch}{1.5} %1.5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\date{}

\title{Active Learning}
\author{\\ \Large{John Ryan}
\\ Computer Science
\\
\\
\\
\\ University of Oxford
\\
\\ \\
Trinity 2021
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\DeclareMathOperator{\E}{\mathbb{E}}

\begin{document}
% Adjust logo positions here
\AddToShipoutPicture*{\BackgroundPicturea{Logos/logo2.png}{0.7in}{5.8in}}

\thispagestyle{headings}
	\maketitle
\FloatBarrier
\pagenumbering{roman}


\thispagestyle{empty}
\begin{abstract}

Active Learning is an use

\keywords{Keyword1 - Keyword2 - Keyword3}
% \vspace{-10mm} %To remove added white space after
\end{abstract}
\tableofcontents
\thispagestyle{plain}
\listoffigures
\listoftables
\listofalgocfs
\listoftheorems

\chapter*{List of Abbreviations}
\begin{abbreviations}
    \item[GP] Gaussian Process
    \item[GPC] Gaussian Process Classification
    \item[BALD] Bayesian Active Learning by Disagreement
    \item[ELBO] Evidence Lower Bound
    \item[ML] Machine Learning
\end{abbreviations}

\chapter{Introduction}
\pagenumbering{arabic}

Active learning is an area of machine learning where we are attempting to select which unlabelled datapoints we believe would be best to learn the label of. It is important because in many important use cases (for example, medical data), it is often either quite costly or just simply impossible to collect an incredibly large dataset. However current approaches to do it suffer from issues related to quality of aquired datapoints, model requirements and computational issues.

Bayesian ML models are the standard ways of building machine learning models which can calculate uncertainty well, however many of the standard approaches to doing so (MC Dropout) can give suboptimal performance, for example requiring a substantial amount of sampling. Gaussian Processes are a ML approach which give very good uncertainty performance, and new results in the field [DUE] have shown successful training of Variational Gaussian Processes with feature extractors and demonstrate that they maintain good uncertainty performance.

\section{Motivation}

Active Learning is an active area of Machine Learning where we are trying to select (unlabelled) data points in an attempt to maximise some objective.
\section{Aim and Objectives}

The aims of this report is to

\begin{itemize}
    \item Give an outline of the various methods for Active Learning which have been published
    \item Compare these methods in a reproducible and standardised fashion
    \item Extend and combine some of these approaches with other ML research to improve Active Learning performance in certain situations.
\end{itemize}
\section{Project Report Outline}
The remainder of this report is organised as follows:
\begin{itemize}
    \item[] \textbf{Chapter} \hyperref[Chap2]{\textbf{2}} --- Defines active learning, and introduces the different active learning methods from the literature.
    \item[] \textbf{Chapter} \hyperref[Gaussian Processes]{\textbf{4}} --- introduces Gaussian Process based models
    \item[] \textbf{Chapter} \hyperref[Chap5]{\textbf{5}} --- Combines BatchBALD and GPC
\end{itemize}
>

\chapter{Active Learning Methods}
\label{Chap2}

In a common active learning formulation we have the following setup.

\begin{itemize}
    \item $X_{pool}$ this is the distribution of the dataset which we have to work with in our data pool.
    \item $X_{true}$ this is the true real world distribution.
\end{itemize}

A common (and sensible) assumption is to assume that these distributions are the same.

Our machine learning models which we use to model a problem constrain the set of possible functions which we can represent and learn, this is how we imbue the problem with our prior beliefs about the nature of the problem.

- There are hard constraints (eg clipping the output of the model), which even given an unlimited amount of data our model can not possibly "learn around"
- There are softer constraints (priors over weights in a layer), which our model should be able to learn correctly given sufficient new data even if our prior is poor*.

This paramaterisaton of our models is very important, this is an assumption we are making about our problem.

An important observation is that the dataset we are using for training is (unless performing very simple active learning approaches eg random acquisition) not going to be the same as the true dataset which we are working with. 

This violates a very common assumption that we assume for convergence in many ML methods, and is something worth keeping in mind. This statistical bias is looked at in the following paper \cite{farquhar2021statistical}.

When working with Active Learning we are also normally working with much smaller datasets than is standard in ML, our models which are normally over paramatised to begin with become even more so, this can lead to training issues.

\section{Random}

The most straightforward acquisition function is to randomly sample from the poolset. This is a surprisingly strong baseline for for many datasets. For example with an unbalanced dataset random aquisition can fall behind as it will on average select points in the same proportion as which they occur in the pool. Here we have a comparision between unweighted MNIST and weighted MNIST with random aquisition.

\includegraphics{todo}


\section{BALD}
Bayesian Active Learning by Disagreement (BALD) \cite{houlsby2011bayesian} is an active learning method which is based on selecting datapoints which maximise the mutual information between the datapoints class output and the models parameters. The paper details how we can instead of computing this in the paramater space which would be computationally prohibative in many cases, we can compute the same objective in the output space.

$${\argmax}_x H \left[ \theta | D \right] - \mathbf{E}_{y \sim p(y | x, D)} \left[ H\left[ \theta | y, x, D\right]\right]$$

$${\argmax}_x H \left[ y | x, D \right] - \mathbf{E}_{\theta \sim p(\theta | D)} \left[ H\left[ y | x, \theta \right]\right]$$


An issue which we run into when using BALD in the case where we want to select multiple datapoints to aquire at the same time is the fact that selecting the $n$ points with the greatest individual information content will not necessarily give you the $n$ points which jointly have the greatest information content. This can be made very obvious by considering the case when there is duplicates in the dataset. If the duplicated point is in the top n points as decieded by the BALD score, having both of the points in your aquired batch will give you no more information than just having one of them, you would be better served by selecting any other point instead of these points.

\section{BatchBALD}
BatchBALD \cite{kirsch2019batchbald}, is an extension of BALD designed especially to deal with the issues which we run into when using BALD to aquire batches of points.

To do this the objective function is modified, the original BALD objective is retained by setting $n = 1$ .

The BatchBALD score function is as follows.


\begin{definition}[BatchBALD]
    $$s_{BatchBald} (x_1, \ldots, x_n) = H(y_1, \ldots, y_n) - E_{p(f)}\left[H(y_1, \ldots, y_n | f)\right]$$
\end{definition}


\begin{definition}[Submodularity]
    A function $f : P(X) \rightarrow \mathbf{R}$ is sub modular if for all $A,B \subseteq X$ we have the following.

    $$f(A) + f(B) \geq f\left( A \cup B \right) +  f\left(A \cap B \right) $$
\end{definition}

The authors of the paper prove that the objective function that they specify is \textit{submodular}, which enables a greedy $1 - \epsilon$ approximation algorithm for selecting the optimium batch.



As the variables are independent conditioned on the value of the function, the right hand side of the equation factors into a sum over conditional expectations for each individual datapoint. The computationally difficult part of this objective is the computation of the joint entropy as this does not factor similarly. In fact, the summation required to compute this value is exponential in the number of datapoints we are currently considering. Estimating the entropy of a discrete distribution is something which we will investigate later \hyperref[sec:Entropy]{here}.


The authors of the paper address this issue by using sampling to estimate the joint entropy for larger batch sizes. The BatchBALD paper uses Bayesian Neural networks (BNNs) as their model. They are then able to sample from the weight distribution once to obtain a function draw for all possible inputs. This is not an ability we have when using GPCs, which we will come back to later.


\section{Joint Entropy}

We can also simply use the joint entropy alone as the active learning objective

$$s_{Entropy} (x_1, \ldots, x_n) = H(y_1, \ldots, y_n)$$

\begin{proof}[$s_{Entropy}$ is submodular]
    Proof.
    Let $A_x,B_x \subseteq D_{pool}, C_x = A_x \cap B_x$
    
    $\hat{A} = A_y \setminus C_y, \hat{B} = B_y \setminus C_y$
    \begin{align*}
        s_{Entropy} (A_x) + s_{Entropy} (B_x) &= H(A_y) + H(B_y)\\
        &= H(C_y \cup \hat{A}) + H(C_y \cup \hat{B})\\
        &= H(\hat{A} | C_y) + H(\hat{B} | C_y) + 2 H(C_y)\\
        &\geq H(\hat{A}, \hat{B} | C_y) + 2 H(C_y)\\
        &= H(\hat{A}, \hat{B}, C_y) + H(C_y)\\
        &= H(A_y \cup B_y) + H(A_x \cap B_x)\\
        &= s_{Entropy} (A_x \cup B_x) + s_{Entropy} (A_x \cap B_x)
    \end{align*}

\end{proof}

The fact that this objective is submodular enables us to use the same greedy $1 - \frac{1}{\epsilon}$ approximation algorithm as used above.

A justification for the use of this objective can be seem as attempting to minimise the entropy over the remaining items in the pool given the batch which we select.


$$\min_{B} H( \left(P \setminus B\right) | B) = \min_{B} H(P) - H(B) = \min_{B} - H(B) = \max_B H(B)$$

If we assume that the samples in the poolset are samples from the true distribuiton, we can view this as attempting to minimise the entropy of the output variables maximally fast, this is in contrast with the BALD objective which attempts to minimise the entropy of the paramaters maximially fast.

In the next chapter we will discuss some reasons while this may be preferable when using a Gaussian Process Classifer.

\chapter{Bayesian Machine Learning Models}

Here we will detail some Bayesian Machine Learning models, in particular focusing on 2 types.

\begin{itemize}
    \item \hyperref[sec:GaussianProcesses]{Gaussian Processes}
    \item \hyperref[sec:BNNs]{Bayesian Neural Networks}
\end{itemize}

\section{Bayesian Neural Networks}
\label{sec:BNNs}


Bayesian Neural Networks

\section{Gaussian Processes}
\label{sec:GaussianProcesses}


\begin{definition}[Gaussian Process]
    A Gaussian processes is a collection of random variables, any finite number of which have a joint Gaussian distribution. \cite[]{rasmussen2003gaussian}
\end{definition}


We can completely specify a Gaussian Process over a space $\chi$ by giving a mean function $m(\cdot), m : \chi \rightarrow R$, and covariance (kernel) function $k(\cdot, \cdot), k: \chi \times \chi \rightarrow R$.


For $k$ to be a valid covariance funciton we need for any evaluation of the covariance matrix to be a symetric positive definite matrix (as all covariance matrices of random variables must be S.P.D).

\begin{definition}[Positive Definte Matrix]
    A positive definite matrix is a matrix which has positive eiginvalues.
\end{definition}

\subsection{As distributions over functions}

We can view a Gaussian Processes as a distribution over functions. The space of functions which we can represent by a particular Gaussian Process is paramaterised by the kernel function which we are using.

We can understand this by investigating the interpretation of kernels as Repreducing Kernel Hilbert Spaces.





Gaussian Processes have a number of useful properties such as being closed under conditioning and marginalisation.

For the covariance function to define a valid MVN for all inputs the covariance function must be positive definite.

A positive definite kernel function can be represented by a feature space where the value of the kernel between 2 values can be computed by taking the inner product of the feature space of the 2 values.

Certain kernel functions such as the radial basis kernel, is represented by an infinite dimensional feature space. This infinite dimensional feature space enables it to model arbitarily complex functions. Polynomial or linear kernels do not enable this level of expressibility.

We can use Bayes rule to update the covariance and mean function from our prior, to the postior given noisy of exact data. The complexity of doing this (given arbitary kernels) is bounded by matrix inversion and  multiplication of a matrix of size of the number of datapoints in which we are using. The complexity of which is sub cubic, however in general the matrix multiplication algorithms we use practically are of the cubic variety).

This can be done as follows: 

(TODO) put update equations.

\subsection{Scaling to large data}

Unlike other methods we use often in machine learning, such as neural networks which have linear complexity in the size of the dataset we have superlinear complexity.

\subsubsection{Inducing Points}

In inducing point models, we instead of using all of the datapoints in our model. We instead use a smaller number of datapoints as an approximation to the true model. Various approaches exist for selecting the points we keep in our approximation.

One of the most sucessful approaches is to treat these inducing points as model parameters which we attempt to learn by minimising some objective.

Minimising the evidence lower bound (ELBO) is a common and principled object for this task.

Minimising this objective minimises the KL Divergence between the true output distribution and the approximate distribution we are learning.

\subsubsection{Random Fixed Features}

(Todo)





\section{Gaussian Process Classification}


To use Gaussian Processes for classification we have several issues to overcome.

The output of a Gaussian Process is a Multivariate Normal, this is a continous distribution over the real space, however in the classification setting we wish to have a categorical output. Which means we need to have a distribution over the the simplex.

We can in theory use any link function to convert a sample from our MVN in $R^n$ to a sample over the simplex, different link functions have different properties computationally and statistically.

\subsection{Link Functions}

The link function between the latent function (our GP) and the output of our classifier is again another parameter of the model.

\subsubsection{Logit}

The logit is the standard link function for the vast majority of GPC use cases. The use of the logit implicitly makes our GPC model obey the independence of irrelevant alternatives axiom from decision theory. 


$$p\left( y = k | f\right) = \frac{e^{f^T w_k}}{\sum_{i=1}^{K} e^{f^T w_i}}$$


The marginal distribuiton using this link function does not have.


\subsubsection{Probit}

The probit is a different link function which is normally defined for the binary classification case as the inverse of the CDF of a Gaussian. We can generalise this link function as follows.

$$p\left(y = k | f\right) = \mathbf{1}_{argmax_i(f_i) = k}$$


The probit function is easier to work with anallytically, and great use of this is made in the binary classification case \cite{houlsby2011bayesian}. However it is clear from the defintions that a single sample draw from a GPC using a probit will not give an accurate assement of the confidence of the model.


\label{sec:Integration}

If we use probit as our link function, we can take advantage of the fact that it is possible to exactly compute the marginal to get exact results.

If we can compute $p(y=k)$, we can also compute the joint entropy exactly (unlike in the case with an arbitary link function), however the complexity of this exact computation is very high.



To perform exact inference of a Probit GPC, we must perform integration of a Gaussian R.V. This is a well studied problem in the literature. [cite].


We wish to find the probability that a certain element of a MVN is the largest element. This corresponds to integrating the Gaussian over the subset of the space where this coordinate is the largest.

$$P(X_C > X_1 \land \ldots \land X_C > X_n) = \int_{x_c \in \{ -\infty, \infty \} } \ldots \int_{x_n \in \{ -\infty , x_c \}} p(x) dx$$

This can be interpreted as integrating on one side of several hyperplanes.

These planes are of the form $x_c = x_i$

We can perform a linear transformation on this space to project it to a subspace of dimension $n-1$, and align each of the hyperplaces with an axis.

$Z = TX$.

$Z \sim N(T \mu, T \Sigma T^T)$


If we are doing this over multiple datapoints, we can transform each of the subsets of variables via the method above.

After performing this transformation we have a standard orthant integral which we can compute. Algorithms exist for computing this exactly in $O(p^2 2^p)$ using recursive integration and subspace projection. \cite{orthant}


However the number of dimensions which we are performing this integration over is $O(dc)$. With the exponential complexity in the number of dimensions this is an infeasible method.



\subsection{Minimising entropy over functions}

When using a softmax link function


\section{Training}

Using a straightforward Gaussian Process with no hyper parameters, for regression there is nothing to train. We simply have the exact equations to compute the posterior distribuiton at any possible input. In contrast to many optimization problems our output for any particular value is not a point estimate, it is a distribuiton so our objective should take this into account.


\subsection{KL Divergence}

The KL Divergence is a measure of how different 2 probability distribuitons are, (it is not symmetric so it is not strictly a distance functions). It has many useful properties and unlike shannon entropy it has a sensible and natural extension to continous random variables.

\begin{definition}[KL Divergence]
    For 2 probability distributions $P, Q$ defined on the same probability space $\chi$
    $$D_{KL} \left( P || Q \right) = E_{x \sim P(x)}  \log \left( \frac{P(x)}{Q(x)}\right)$$
\end{definition}



It is often quite difficult to compute the \textbf{KL Divergence}, as often we don't have direct access to one of the distribuitons which we are attempting to calculate it with.

The Evidence Lower Bound is a tractable objective which it is much easier for us to compute than the KL divergence and maximising this objective is equivalent to maximising the KL Divergence.

\begin{definition}[ELBO]
    
\end{definition}


\begin{proof}[Maximisng the ELBO will minimise the KL Divergence]
    \begin{align*}
        D_{KL} \left( Q || P \left(L | X\right) \right) &= 
    \end{align*}
\end{proof}


\chapter{vDUQ / DUE}

vDUQ / DUE \cite{vanamersfoort2020uncertainty} is a recent model which deals with some of the issues with using deep kernel learning with GPCs. DUE uses an inducing point GP to enable it to scale to large amounts of data.

This model is designed to have good uncertainty properties by enforcing sensitivity and bounding the amount by which any 2 inputs can diverge in the feature space. These properties are maintained by the additon of residual connections to the model and by performing spectral normalisation to bound the lipshitz constant of the layers in the feature extractor.

\section{Spectral Normalisation}

\begin{definition}[Lipshitz Constant]
    Given a function $f: X \rightarrow Y$, where $(X, d_x)$ and $(Y, d_y)$ are metric spaces. A lipshitz constant of the function $f$ is a value of $k$ such that.

    $$\frac{d_y(f(x_1), f(x_2))}{d_x(x_1, x_2)} \leq K, \forall x_1, x_2 \in X $$
\end{definition}

Some times we refer to \textit{the lipshitz constant} of a function, this is normally taken to mean the smallest value $k$ which is a lipshitz constant.

In our particular problem we are dealing with spaces in $\mathbf{R}^n$. The metric on this space which we will be using is the $L_2$ norm, (also known as the euclidean distance).

The lipschitz constant is a measure of how far apart 2 outputs to the function can be in terms of their inputs.

In the case where $f: R^n \rightarrow R^m$ and $f$ is a linear transformation we get a familar result.

\begin{align*}
    \min_k  \forall x_1,x_2 \in X \frac{d_y(f(x_1), f(x_2))}{d_x(x_1, x_2)} \leq K &= \min_k \forall x_1,x_2 \in X \frac{ L_2(Ax_1 - Ax_2)  }{ L_2(x_1, x_2)} \leq K \\
    &=  \min_k \forall x_1,x_2 \in X  \frac{ ||A(x_1 - x_2)||_2  }{ ||x_1- x_2||_2 } \leq K \\
    &=  \min_k \forall x \in X  \frac{ ||Ax||_2  }{ ||x||_2 } \leq K \\
\end{align*}

After the substitution we obtain an expression which from linear algebra we know is the matrix norm of $A$ induced by the vector norm which we have chosen, in this case the $L_2$ norm. The induced matrix norm by the $L_2$ vector norm has the nice property that it is the \textit{largest singular value} of the matrix which is also the square root of the largest eigenvalue.

For all of the linear layers in our model we now know how we can compute the respective lipschitz constants.

\begin{theorem}[Lipschitz constant of function compositions]
    If $lip(f) = a$ and $lip(g) = b$, then $lip(f \circ g) \leq a * b$
\end{theorem}

It turns out that the majority of activation functions which we use in our neural networks have a lipschitz constant of $\leq 1$, eg relu. This enables us to compute an upper bound of the lipschitz constant of our model by computing the lipschitz constant of all of the indiviudal layers and taking the product of those constants.



\subsection{Power Method}

Directly computing the largest singular value via computing the Singular Value Decompoisition of each linear transform is quite computationally prohibative, however there is a way in which we can approximate this value quite cheaply and accurately.

The \textbf{power method} is defined by a recurrence relation.

\begin{definition}[Power Method(for square matrices)]
    $$v_{k+1} = \frac{Av_k}{||Av_k||} $$
\end{definition}

This recurrence relation will converge if 2 conditions are met.

\begin{itemize}
    \item $A$ has a strictly largest eigenvalue
    \item $v_0$ has a non zero component in the direction of the largest eigenvalue
\end{itemize}

We compute the eigenvalue from the rayleigh quotent from our approximate maximum eigenvector. $\frac{v_k^T A v_k}{v_k ^ T v_k}$

If we chose a starting vector $v_0$ with uniform probability over all possible directions the second condition will be fullfilled with a probability of $1$.

The convergence ratio of this algorithm is $\frac{|\lambda_1|}{|\lambda_2|}$, so the convergence rate depends on the size of the second largest eigenvalue also.


To extend this to work with non square matrices we can either compute $A^TA$ or $AA^T$, use the power method to calculate its maximum eigenvalue and this is the spectral norm of $A$.
There is another approach which enables us to bypass this computation by instead of maintaining just 1 eignevector, we maintain a left and a right domainant eigenvector

\begin{definition}[Power Method(for non square matrices)]
    $$v_{k+1} = \frac{A^Tu_k}{||A^Tu_k||}, u_{k+1} = \frac{Av_k}{||Av_k||} $$
\end{definition}

Then with these appoximate eigenvectors we can approximate the singular value with $u^T A v$

To bound the lipschitz constant of a linear layer to a values, call this value $\sigma$. We can calculate the spectral norm $\alpha$, check if $\alpha > \sigma$. If this is true, we muliply the linear layer by $\frac{\sigma}{\alpha}$.

To maintain this during training we are required to compute this value during every forward pass. Due to the fact that given a sufficently small learning rate we expect that the change to linear layer will be small, if we use the previous forwards passes final vector(s) as our starting vector instead of randomly restarting we can greatly speed up the convergence of the recurrence relation. We find that empircally we can actually get very good performance with a single iteration of this method at each stage.


\section{Residual Connections}

Residual connections were introduced in by [cite]. One of their origianl 


\section{Modifications}

While DUE is an extremely powerful model, some modifications were required to help the performance and stability of the model when working with small amounts of data. 

\subsection{Natural Gradient Descent}

Natural Gradient Descent is an optimization method which is designed for when we are attempting to optimise some objective of a distribution. The use of this method greatly improved the rate of convergence and overall performance. \cite{NGD}


\begin{definition}[Gradient Descent]
    Gradient Descent is an iterative optimization algorithm where we modify the paramter in the direction which has the steepest gradient with respect to our objective function.
    $$p_{k+1} = p_{k} -  \alpha \nabla L(p_k)$$
\end{definition}

With a well chosen hyperparameter $\alpha$ and a convex loss function $L$ we can obtain convergence guarentees. [cite]. Many machine learning problems which we use gradient descent for do not obey the constraints which we require for guarentees of convergence [], empircally however the performance achieved by gradient descent and its derivates is impressive even these problems in many cases.

Instead of applying the gradient descent algorithm to the paramaters in the eucliedean geometry of the paramater space, instead we can use the geometry of the likelihood space paramaterised by our chosen parameters to enable superior optimization. This makes the optimisation independent of the paramaterisaton of the distribuiton and only related to the distribuiton induced by the the parameters.
 
To derive the update equations we will need to define some objectives and take some taylor expansions.

The first thing we will define is out loss function. $q$ is the true distribuiton which we are attempting to approximate, $p_\theta$ is our parameterised distribution.

$$L \left(\theta \right) = D_{KL} \left(p_\theta || q \right) $$

We wish to choose a descent direction which will minimise our loss function, much like in standard gradient descent. Normally in gradient descent we bound the magnitude of the step of the gradient by the eucliedean length of the descent vector, we change the constraint in natural gradient descent to instead bound the $KL$ divergence between $p_\theta$ and $p_{\theta + \epsilon}$.

\begin{align}
    \epsilon^\prime &= \argmin_{\epsilon \, s.t \, D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) \leq k} L \left( \theta + \epsilon \right) \\
\end{align}

We can take the first order taylor expansion of $L\left(\theta + \epsilon\right)$ around $\theta$.


\begin{align*}
L\left(\theta + \epsilon\right) &\approx L\left( \theta \right) + J_L \left( \theta\right) \left( \left( \theta + \epsilon \right) - \theta \right)  \\
&\approx L\left( \theta \right) +  J_L \left( \theta\right) \epsilon \\
\end{align*}

Given the constraints above we have we can rewrite our objective using the method of lagrange mulitpliers as below.

\begin{align}
    \epsilon^\prime &= \argmin_{\epsilon \, s.t \, D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) \leq k} L \left( \theta + \epsilon \right) + \alpha \left( D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) - k\right) \\
    &\approx \argmin_{\epsilon \, s.t \, D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) \leq k}  L\left( \theta \right) +  J_L \left( \theta\right) \epsilon  + \frac{1}{2} \left(\epsilon \right)^T H_L \left(\theta \right) \left( \epsilon\right) + \alpha \left( D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) - k\right) \\
\end{align}

Now we can apply a taylor expansion to $D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right)$ around $\theta$. $\theta^\prime = \theta + \epsilon$


\begin{align}
    D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) &\approx  D_{KL} \left(p_\theta || p_{\theta} \right) + \left(J_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right) \right) \epsilon + \frac{1}{2} \epsilon^T \left( H_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right) \right) \epsilon
\end{align}

The KL Divergence between a distribtuion and it's self is $0$ so the first term disappears.


Lets now deal with the second term. The KL Divergence is a positive function, and we know that as we mentioned above it is $0$ when the 2 distribuitons are equal. Then second term is the Jacobian of the KL Divergence between 2 paramaterised distributitons, \textit{equaluated when the parametrs are equal}. So we are taking the Jacobian of a function evaluated at a global minimum, which implies that $J_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right)  = 0$. This makes the second term disappear.


\begin{align}
    D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) &\approx  \frac{1}{2} \epsilon^T \left( H_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right) \right) \epsilon
\end{align}


We can expand out the matrix in the reamining quadratic term to obtain that it is equal to the negative fisher information matrix.

\begin{align}
    H_{D_{KL} \left(p_\theta || p_{\theta^\prime} \right)}\left( \theta \right)_{ij} &= J \left( J \left( E_{x \sim p_\theta(x)}  \log \left( \frac{p_\theta(x)}{p_{\theta^\prime}(x)}\right)\right)^T \right)_{ij} \left( \theta \right) \\
    &= \frac{\partial}{\partial \theta^\prime_i} \frac{\partial}{\partial \theta^\prime_j} E_{x \sim p_\theta(x)}  \log \left( \frac{p_\theta(x)}{p_{\theta^\prime}(x)}\right) \left( \theta \right) \\
    &= E_{x \sim p_\theta(x)} \frac{\partial}{\partial \theta^\prime_i} \frac{\partial}{\partial \theta^\prime_j}  \log \left( \frac{p_\theta(x)}{p_{\theta^\prime}(x)}\right) \left( \theta \right) \\
    &= - E_{x \sim p_\theta(x)} \frac{\partial}{\partial \theta^\prime_i} \frac{\partial}{\partial \theta^\prime_j}  \log \left( p_{\theta}(x) \right)  \\
    &= F \\
\end{align}

Where $F$ is the defintion of the Fisher information matrix.


\begin{definition}[Fisher Information Matrix]
    $$\mathcal{I} \left( \theta\right)_{ij} = - E \left[ \frac{\partial^2}{\partial \theta_j  \partial \theta_i }  \log p \left(x  \right)\big| \theta \right]$$
\end{definition}

Returning to our objective from earlier.

\begin{align}
    \epsilon^\prime &\approx \argmin_{\epsilon \, s.t \, D_{KL} \left(p_\theta || p_{\theta + \epsilon} \right) \leq k}  L\left( \theta \right) +  J_L \left( \theta\right) \epsilon  + \alpha \left( \frac{1}{2} \epsilon^T F \epsilon - k\right) \\
\end{align}

To find the best value of $\epsilon$, we can take the derivate of the objective with respect to $\epsilon$ at set it to $0$.


\begin{align}
   \frac{\partial}{\partial \epsilon} \left( L\left( \theta \right) + J_L \left( \theta\right) \epsilon + \alpha \left( \frac{1}{2} \epsilon^T F \epsilon - k\right) \right) &=  \frac{\partial}{\partial \epsilon} \left( J_L \left( \theta\right) \epsilon + \frac{\alpha}{2} \epsilon^T F \epsilon  \right) = 0\\
  0 &=  J_L \left( \theta\right)^T + \frac{\alpha}{2}  \left(F^T + F \right)\epsilon\\
\end{align}

Assuming some smoothness condictions we get that the order of the partial differentation doesn't impact the Hessian, and in turn the fisher information matrix. $F = F^T$

\begin{align}
   0 &=  J_L \left( \theta\right)^T + \alpha F \epsilon\\
   - J_L \left( \theta\right)^T &=  \alpha F \epsilon\\
  \epsilon &=  - \frac{1}{\alpha} F^{-1} J_L \left( \theta\right) ^T
\end{align} 

We have now arrived at the update equations which we can use as our optimisation method.


\begin{definition}[Natural Gradient Descent]
    Natural Gradient Descent is similar to gradient descent expect inplace of the the gradient we use the natural gradient defined as $\nabla^{\prime} =  F^{-1} J_L \left( \theta \right)^T$
\end{definition}






\chapter{Investigations}
\label{Chap5}



\section{Comparision of BALD and Entropy for GPC}

The motivation for the BALD objective in the original paper where it was introduced \cite{houlsby2011bayesian}, is to select the point which maximally reduce the uncertainty over the hypothesis space.


For datapoints which are far from the datapoints which we have seen, the covariance decreases rapidly. 




\section{Sampling}

Given the difficultly of performing exact computation of the quantities required for the BatchBALD objective, the general approach is to use methods which rely on sampling.

In comparision with the approach performed in the BatchBALD paper where sampling is done over the weights of the Bayesian Neural Network, when we have a Gaussian Process as our output. It is not as straightforward to take a sample function from our postior. If we are to take a sample function output over a set out points, we would need to compute the joint distribution of these points and then take a sample from this distribution and pass this through our link function.
The complexity of performing this is cubic in the size of the pool, this complexity is unacceptably high as we are often working with quite large pool sizes.

This means that we can not use the same computational tricks used previously to speed up these calculations, however we can take advantage of other properties of Gaussian Processes to make some improvements in this aspect.


\section{Entropy}
\label{sec:Entropy}

\begin{definition}[Entropy]
    For a discrete r.v $X$ the entropy is defined as $$H(X) = - \sum_{x \in X} p(x) \log p(x) $$
\end{definition}


We require the ability to compute the joint entropy of a large number of random variables to compute several of the score functions which we are evaluating.

\subsection{Gaussian Process Classifer}
As we are using a Gaussian Process Classifier, the random variable of which we are trying to estimate the value of is of a known form.


$$ \mathbf{f} \sim N(\mu, \Sigma) $$
$$ \mathbf{p} =  \sigma \left(f \right) $$
$$ X_i \sim Cat(p_i) $$

The distribution $p(x)$ does not have a closed formula. We can estimate $p(x)$ via estimating the intergral via monte carlo intergration.

When we are considering larger and larger pool sizes the number of possible states of our random variable grows at an exponential rate.

With $C$ classes the entropy sumation will have $C^n$ terms in it. 

As we have that $ X_i \perp\!\!\!\perp X_j | f$, for a given likelihood sample we can store the exponential number of terms in a factored form. We for a sample $f$, we can compute $p(x_i = c | f)$ for each of the datapoints. To reconstruct the joint distribution for this function sample we take the product of the terms and relevant categories.


$$ p(X) = p(x_1, \ldots, x_n) = \E_{l \sim p(l)}  p(x_1, \ldots, x_n | l) = \E_{l \sim p(l)} \prod_{i=1}^n  p(x_i| l)$$

\subsection{Estimating the entroy}

The are 2 different approaches we can use in the estimation.

\subsection{Taking samples from the distribution}

We can take samples from the distribution, and then use these samples to estimate the entropy.



\subsubsection{Plugin Estimator}
The most straightforward approach is to use the \textit{plugin estimator}. This approach is to take $k$ samples from the distribution of interest, calculate the observed probability of each class and compute the entropy.

As we define the term in the entropy summation as $0$, when the probability is $0$. Using this approach bounds the number of terms in the summation by $k$.

$$p_j = \frac{1}{k} \sum_{i=1}^k \mathbf{1}_{X = j} $$

$$\hat{H}_k = - \sum_{i=1}^{c} \hat{p}_i \log{\hat{p}_i}$$


Some of the convergence properties of the plugin estimator are below.
$$\E \left(H - \hat{H}_k \right)^2 = O \left ( \frac{1}{k} \right) $$

$$ \frac{\sqrt{K}}{\sigma} \left(H - \hat{H}_k \right) \sim \mathbf{N} \left(0, 1 \right) $$



\subsubsection{LP Estimator}

Using an \textit{LP Estimator} [cite] we can estimate the the entropy with far less samples in comparision to using a plugin estimator. The required number of samples from the distribution to estimate the entropy well is \textit{sublinear}, of the order $O \left( \frac{d}{\log{d}}\right)$, where $d$ is the number of values in the domain which have positive support. However the amount of values with support in our problem is going to be exponential as our final layer is a softmax, which means that $d = c^n$.


\subsection{Sampling from the outer summation}

As we can actually compute the values $p(x)$ without sampling from the final categorical distribution, we can take advantage of this fact to improve the performance of our estimator. We can directly obtain the probabilities after the softmax. This enables us to perform the estimation in 2 steps.

\begin{itemize}
    \item Sample from the outer summation to see which values of $p(x)$ we are required to compute.
    \item Estimate $p(x)$ directly from the output of the softmax.
\end{itemize}

We can also take advantage of the fact that the output variables are conditionally independent given a function sample. This enables us to sample from $p(x)$ without requiring an exponential amount of computation by sampling from the distribution in the following the following sequence of steps.

\begin{itemize}
    \item Sample from the likelihood distribution $p(l)$
    \item Put this sample though the softmax
    \item As the outputs of the different variables are conditionally independent given $l$, we sample once from the categorical distribuiton for each variable.
    \item This is a sample from $p(x)$
\end{itemize}

\chapter{Computation}
\label{sec:Computation}


\chapter{Implementaion}
\label{sec:Implementaion}

All of the experiments referenced here can easily be replicated using the codebase for this project which is linked and details in the appendices.

The code for this project is written in Python and makes significant use of the frameworks \cite[Pytorch]{NEURIPS2019_9015} and \cite[GPytorch]{gardner2018gpytorch}.





\section{Computational Complexity}

In this project we also endevour to minimise the computationally complexity of the operations which we need to perform to compute the quantities involved to make this project of practical use.


\subsection{Sampling from an Multivariate Distribution}


During the creation and sampling of the Multivariate distributions we are required to compute several quantities.

We generate samples from MVNs by generating samples from the identity mvn, and then transforming these samples.

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwFunction{CreateSamples}{CreateSamples}
    \CreateSamples{$\mu, \Sigma$}{
    \KwResult{Sample from $ N \left(\mu, \Sigma \right)$ }
    $n = len(\mu)$\;
    $v = array[n]$\;
    $i = 0$\;
    \While{$i < n$}{
        $v[i] = mvnsasmple()$\;
        $i++$\;
    }
    $L = cholesky(\Sigma)$\;
    \KwRet $\mu + L v $\;
    }
    \caption{Sampling from a Multivariate Gaussian}
\end{algorithm}

To sample from a distribuiton of size $n$, if the cholesky distribution is already computed this requires $O(n^2)$ operations. If it is not it requires $O(n^3)$. This encourages us to attempt to minimise the number of distint times where we must compute the cholesky distribuiton.


\subsection{Sampling over our candidate batchs}

Computing samples for each possible candidate batch at each aquisition naively would lead to a complexity at each stage of $O\left(n k (d \times c)^3 \right)$ for sampling, where $k$ is the number of samples, $c$ is the number of categories, $n$ is the number of points in the pool and $d$ is the size of the current candiate batch.

If we cache the cholesky distributions we can reduce the complexity to $O(n (d \times c)^3  + n k (d \times c)^2)$.

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwFunction{CreatePoolSamples}{CreatePoolSamples}
    \CreatePoolSamples{$\mu, \Sigma$}{
    \KwResult{$K$ samples from each of the $N$ candiate batches}
    $v = array[N][K][D][C]$\;
    $i = 0$\;
    \While{$i < N$}{
        $j = 0$\;
        \While{$j < K$}{
            $v[i][j] = CreateSamples(\mu_{i}, \Sigma_{i})$\;
            $j++$\;
        }
        $i++$\;
    }
    \Return{$v$}\;
    }
     \caption{Sampling from all possible batches}
\end{algorithm}

This level of complexity is unacceptable, however we can take advantage of the structure of our model to improve the efficency.

As the Gaussian Process Model we are using is an \textit{independent multitask model}. This means that our covariance matrix is a block diagonal. We can take advantage of this to instead of working with covariance matrices of size $ cd \times cd $, we can work with a batch of matrices of size $c \times d \times d$ in our computaion.


This reduces the complexity of the sampling to $O(n c (d)^3  + n k c (d)^2)$.

\subsection{Conditional Distributions}

We can reduce the amount of computation required by sharing computaion between datapoints.

We can do this as follows first by sampling from the current batch, then sampling from the conditional distribuiton of each of the datapoints. The size of the current batch will increase as our aquisitions progress, the size of the consitional distribuitons will not.


\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwFunction{CreatePoolSamplesEfficent}{CreatePoolSamplesEfficent}
    \CreatePoolSamplesEfficent{$\mu, \Sigma, \mu_{batch}, \Sigma_{batch}$}{
    \KwResult{$K$ samples from each of the $N$ candiate batches}
    $v = array[N][K][D][C]$\;
    $samples = array[K][D][C]$\;
    $a = 0$\;
    \While{$a < K$}{
        $samples[a] = CreateSamples(\mu_{batch}, \Sigma_{batch})$\;
        $a++$\;
    }
    $i = 0$\;
    \While{$i < N$}{
        $j = 0$\;
        \While{$j < K$}{
            $\mu_{cond dist}, \Sigma_{cond dist} = CreateConditional(\mu_i, \Sigma_i, samples[j])$\;
            $v[i][j] = CreateSamples(\mu_{cond dist}, \Sigma_{cond dist})$\;
            $j++$\;
        }
        $i++$\;
    }
    \Return{$v$}\;
    }
     \caption{Sampling from all possible batches}
\end{algorithm}

$CreateConditional$ will require a $O(1)$ matrix vector products each with a complexity of $O(cd^2)$, and we will require the matrix inverse of the current batch covariance matrix. This will be the same over all function calls so we don't need to recompute this value.

The complexity of this algorithm is $O\left(cd^3 + kcd^2 + nkcd^2 \right)$.

The improvement here is to disconnnect the size of the poolset from the cubic $d^3$ term.

The single cubic term can also be removed by performing a rank-1 update on the inverse of the previous batch. This can be performed by using a block matrix inversion identity.

$$\begin{bmatrix}
    A & B \\
    C & D \\
\end{bmatrix}^{-1} = \begin{bmatrix}
    A^{-1} + A^{-1} B \left(D - C A^{-1} B \right)^{-1} C A^{-1} & -A^{-1} B \left( D - C A^{-1}B\right)^{-1} \\
    - \left(D - CA^{-1}B \right)^{-1} C A^{-1} & \left(D - C A^{-1} B\right)^{-1}
\end{bmatrix} $$ 

Substitutioning in our rank 1 format.

\begin{align*}
    \begin{bmatrix}
        A & u \\
        u^T & \alpha \\
    \end{bmatrix}^{-1} &= \begin{bmatrix}
        A^{-1} + A^{-1} u \left(\alpha - u^T A^{-1} u \right)^{-1} u^T  A^{-1} & -A^{-1} u \left( \alpha - u^T A^{-1}u\right)^{-1} \\
        - \left(\alpha - u^T A^{-1}u \right)^{-1} u^T A^{-1} & \left(\alpha - u^T A^{-1} u\right)^{-1}
    \end{bmatrix}
\end{align*}

If we compute the upper quadrant of the result in such an order that we compute matrix vector products until we are left with a outer product, this will give us a $O(d^2)$ matrix inverse update.


This method gives us a complexity of  $O\left(nkcd^2 \right)$ for the entire sampling.


We can once again reduce the amount of computaion required to take $K$ samples from each candidate by taking $S = K // M$ samples from the batch and taking $M$ samples from the conditional distribuitons.


If we do this our complexity will become $O\left(scd^2 + nscd^2 + nsmc\right)$.

\subsection{Creation of GP Outputs required}

To be able to compute the samples as we have above, at each stage of the aquisition we require the covariance of each point in the candidate batch and the pool points. To compute this at each batch batch directly we end up requring $O(ncd^3)$ computation to evaluate the covariance matrices. However there is clearly wasted computation as all of the candidate batches will have an identical submatrix of size $d \times d$.

The approach taken in this project is as follows. We at each stage compute the GP output of the most recently aquired datapoint with each of the datapoints in the pool. The complexity of this operation is $O(nc)$, as we are only computing covariances between pairs of points.

We can then cache these computaions as the aquisitions progress, we can compute all of the possible rank-1 updates to the current batch in time $O(ncd)$, from these stored values. This once again eliminates the cubic term and replaces it with a linear one.


Both the conditional distribuitons and the rank 1 updates are lazily generated to reduce the memory usage of the program, in addition to the computations are dynamically chunked to enable them to fit in memory and to be able to execute with limited RAM. This is essential to obtain good performance using CUDA operations on a GPU, as the amount of VRAM available limits the amount of parallel computaion one can take advantage of.

\subsection{Entropy}



\section{Replication}

To begin with replication of some of the experiments in the BatchBALD to valididate the experimental framework and to have points of comparision to work with.




\section{DUE}

\subsection{Model Outline}


\subsubsection{Training issues}

When training DUE/vDUQ based models on extremely small datasets, an issue which begins to become apparent is the issue of inducing point initialisation. As feature extractor is randomly initialised, the initial inducing points we get obtain by using standard initial inducing point initialisation procuedures can lead to poor training dynamics.





\subsection{Sampling Functions}

As mentioned earlier in the paper sampling a function over the entire pool is computationally prohibative, however we can take a small enough subset of the pool over which we can calculate the joint distribution and take sample function draws.
We can then use these as a numerical test to valididate that the more computationally efficent methods we develop are giving correct results.


\chapter{Experiments}

We have set up some of these experiments to demonstrate the issues with some of the active learning methods detailed in this paper.

The random method should be impacted by how unbalanced the dataset is particuallly.

\section{Replication}

\subsection{Random}

\subsubsection{MNIST (DNN vs BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{Random MNIST}
\end{figure}

\subsubsection{Unbalanced MNIST (DNN vs BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{Random Unbalanced MNIST}
\end{figure}
\subsubsection{Repeated MNIST (DNN vs BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{Random Repeated MNIST}
\end{figure}

\subsection{BALD}

\subsubsection{MNIST (BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{BALD MNIST}
\end{figure}

\subsubsection{Unbalanced MNIST (BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{BALD Unbalanced MNIST}
\end{figure}

\subsubsection{Repeated MNIST (BNN vs DUE)}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{todo.png}
\caption{BALD Repeated MNIST}
\end{figure}
    

\subsection{BatchBALD}

\subsubsection{MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\subsubsection{Unbalanced MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\subsubsection{Repeated MNIST (BNN vs DUE)}

\includegraphics{todo.png}


\subsection{Predictive Entropy}

\subsubsection{MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\subsubsection{Unbalanced MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\subsubsection{Repeated MNIST (BNN vs DUE)}

\includegraphics{todo.png}

\section{DUE}

To attempt to keep the experiments as comparible as possible, the feature extractor architecture used for DUE for this problem was the same architecture as the BNN with minimal modifications to satify the constraints of DUE. This primarily consisted of the additon of residual connections to the models, and the removal of the dropout layers.


\subsection{Training on small datasets}

Training a machine learning model on a small dataset can exaherbate many issues that we run into in machine learning. For example the models we have are even further over parameterised when we are operating in very small datasets.

While DUE has no problems in learning MNIST with more expressive models, eg more traditional ResNets. When using these the performance on very small datasets was reduced significantly, the performance of DUE on small subsets of MNIST was quite significatnly impacted.


\subsection{BALD}

\subsection{BatchBALD}

\subsection{Predictive Entropy}






\renewcommand{\bibname}{Bibliography}
\bibliographystyle{apacite}
\bibliography{Bibliography.bib}

\begin{appendices}
\chapter{Appendix}

\end{appendices}

\end{document}